{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ CardioFusion: Advanced Machine Learning Models\n",
    "\n",
    "## üìã **Project Overview**\n",
    "This notebook implements advanced machine learning models for cardiovascular disease prediction, building upon the baseline models to achieve superior performance through:\n",
    "\n",
    "### üéØ **Advanced Models Implemented**\n",
    "1. **XGBoost** - Gradient boosting with hyperparameter optimization\n",
    "2. **LightGBM** - Fast gradient boosting framework\n",
    "3. **Neural Network** - Deep learning with TensorFlow/Keras\n",
    "4. **Hybrid Ensemble** - Weighted combination of all models\n",
    "\n",
    "### üìä **Dataset Information**\n",
    "- **Source**: Preprocessed CVD_Cleaned.csv (567,606 balanced records)\n",
    "- **Features**: 27 engineered and encoded features\n",
    "- **Target**: Heart Disease (50% No, 50% Yes after SMOTE)\n",
    "- **Split**: 80% Training (454,084), 20% Testing (113,522)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Traditional ML\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Advanced ML Models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Advanced Models Training Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîß TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"üîß XGBoost Version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ **Load Preprocessed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "try:\n",
    "    # Load training and testing sets\n",
    "    train_data = pd.read_csv('train_data.csv')\n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_data.drop('Heart_Disease', axis=1)\n",
    "    y_train = train_data['Heart_Disease'].map({'No': 0, 'Yes': 1})\n",
    "    X_test = test_data.drop('Heart_Disease', axis=1)\n",
    "    y_test = test_data['Heart_Disease'].map({'No': 0, 'Yes': 1})\n",
    "    \n",
    "    print(f\"‚úÖ Training data: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "    print(f\"‚úÖ Testing data: {X_test.shape[0]:,} samples, {X_test.shape[1]} features\")\n",
    "    print(f\"\\nüìä Class Distribution:\")\n",
    "    print(f\"   Training: {(y_train==0).sum():,} No Disease, {(y_train==1).sum():,} Disease\")\n",
    "    print(f\"   Testing:  {(y_test==0).sum():,} No Disease, {(y_test==1).sum():,} Disease\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data files not found. Please run data_preprocessing.ipynb first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ **Model 1: XGBoost with Hyperparameter Tuning**\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful ensemble method that often achieves state-of-the-art results in structured data problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ TRAINING XGBOOST MODEL\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "xgb_param_grid = {\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',  # Faster training\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"üîç Performing randomized hyperparameter search...\")\n",
    "print(\"   This may take several minutes...\")\n",
    "\n",
    "# Randomized search for faster hyperparameter optimization\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "xgb_random.fit(X_train, y_train)\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Get best model\n",
    "best_xgb = xgb_random.best_estimator_\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"üèÜ Best parameters: {xgb_random.best_params_}\")\n",
    "print(f\"üìä Best CV ROC-AUC: {xgb_random.best_score_:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "y_pred_proba_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'precision': precision_score(y_test, y_pred_xgb),\n",
    "    'recall': recall_score(y_test, y_pred_xgb),\n",
    "    'f1_score': f1_score(y_test, y_pred_xgb),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà XGBoost Performance:\")\n",
    "print(f\"   Accuracy:  {xgb_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {xgb_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {xgb_metrics['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {xgb_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC:   {xgb_metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='#1e3a8a', alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.title('üî¨ XGBoost - Top 15 Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† **Model 2: Neural Network (Deep Learning)**\n",
    "\n",
    "Multi-layer perceptron with batch normalization and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† BUILDING NEURAL NETWORK\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the neural network architecture\n",
    "def create_neural_network(input_dim):\n",
    "    \"\"\"\n",
    "    Create a professional-grade neural network for binary classification\n",
    "    \n",
    "    Architecture:\n",
    "    - Input Layer: input_dim features\n",
    "    - Hidden Layer 1: 128 neurons + BatchNorm + Dropout(0.3)\n",
    "    - Hidden Layer 2: 64 neurons + BatchNorm + Dropout(0.3)\n",
    "    - Hidden Layer 3: 32 neurons + BatchNorm + Dropout(0.2)\n",
    "    - Output Layer: 1 neuron (sigmoid activation)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(128, activation='relu', input_dim=input_dim, name='dense_1'),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        Dropout(0.3, name='dropout_1'),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        Dense(64, activation='relu', name='dense_2'),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        Dropout(0.3, name='dropout_2'),\n",
    "        \n",
    "        # Hidden layer 3\n",
    "        Dense(32, activation='relu', name='dense_3'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        Dropout(0.2, name='dropout_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ], name='CardioFusion_NN')\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "nn_model = create_neural_network(X_train.shape[1])\n",
    "\n",
    "# Display architecture\n",
    "print(\"\\nüèóÔ∏è Neural Network Architecture:\")\n",
    "nn_model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training neural network...\")\n",
    "print(\"   This may take several minutes...\\n\")\n",
    "\n",
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "history = nn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=1024,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba_nn = nn_model.predict(X_test).flatten()\n",
    "y_pred_nn = (y_pred_proba_nn >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "nn_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_nn),\n",
    "    'precision': precision_score(y_test, y_pred_nn),\n",
    "    'recall': recall_score(y_test, y_pred_nn),\n",
    "    'f1_score': f1_score(y_test, y_pred_nn),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_nn)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà Neural Network Performance:\")\n",
    "print(f\"   Accuracy:  {nn_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {nn_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {nn_metrics['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {nn_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC:   {nn_metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Neural Network Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('üß† Neural Network - Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('üß† Neural Network - Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ **Model 3: Hybrid Ensemble (Soft Voting)**\n",
    "\n",
    "Combines all models using weighted soft voting for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ BUILDING HYBRID ENSEMBLE\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Load baseline models\n",
    "baseline_models_dir = 'baseline_models'\n",
    "lr_model = joblib.load(f'{baseline_models_dir}/logistic_regression_model.pkl')\n",
    "dt_model = joblib.load(f'{baseline_models_dir}/decision_tree_model.pkl')\n",
    "rf_model = joblib.load(f'{baseline_models_dir}/random_forest_model.pkl')\n",
    "\n",
    "print(\"‚úÖ Loaded baseline models\")\n",
    "\n",
    "# Create ensemble with weighted voting\n",
    "# Weights based on individual model performance\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic_regression', lr_model),\n",
    "        ('decision_tree', dt_model),\n",
    "        ('random_forest', rf_model),\n",
    "        ('xgboost', best_xgb)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[0.15, 0.30, 0.25, 0.30]  # Higher weights for better performers\n",
    ")\n",
    "\n",
    "print(\"üîß Ensemble configuration:\")\n",
    "print(\"   Models: Logistic Regression, Decision Tree, Random Forest, XGBoost\")\n",
    "print(\"   Voting: Soft (weighted probability averaging)\")\n",
    "print(\"   Weights: [0.15, 0.30, 0.25, 0.30]\")\n",
    "\n",
    "# Train ensemble\n",
    "print(\"\\nüöÄ Training hybrid ensemble...\")\n",
    "start_time = datetime.now()\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "ensemble_metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_ensemble),\n",
    "    'precision': precision_score(y_test, y_pred_ensemble),\n",
    "    'recall': recall_score(y_test, y_pred_ensemble),\n",
    "    'f1_score': f1_score(y_test, y_pred_ensemble),\n",
    "    'roc_auc': roc_auc_score(y_test, y_pred_proba_ensemble)\n",
    "}\n",
    "\n",
    "print(f\"\\nüìà Hybrid Ensemble Performance:\")\n",
    "print(f\"   Accuracy:  {ensemble_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {ensemble_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {ensemble_metrics['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {ensemble_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC:   {ensemble_metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Comprehensive Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results DataFrame\n",
    "print(\"üìä COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'Neural Network', 'Hybrid Ensemble'],\n",
    "    'Accuracy': [xgb_metrics['accuracy'], nn_metrics['accuracy'], ensemble_metrics['accuracy']],\n",
    "    'Precision': [xgb_metrics['precision'], nn_metrics['precision'], ensemble_metrics['precision']],\n",
    "    'Recall': [xgb_metrics['recall'], nn_metrics['recall'], ensemble_metrics['recall']],\n",
    "    'F1-Score': [xgb_metrics['f1_score'], nn_metrics['f1_score'], ensemble_metrics['f1_score']],\n",
    "    'ROC-AUC': [xgb_metrics['roc_auc'], nn_metrics['roc_auc'], ensemble_metrics['roc_auc']]\n",
    "})\n",
    "\n",
    "print(\"\\nüìã Advanced Models Performance:\")\n",
    "print(results_comparison.round(4).to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_comparison['F1-Score'].idxmax()\n",
    "best_model_name = results_comparison.loc[best_model_idx, 'Model']\n",
    "best_f1 = results_comparison.loc[best_model_idx, 'F1-Score']\n",
    "\n",
    "print(f\"\\nüèÜ BEST ADVANCED MODEL: {best_model_name} (F1-Score: {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visual Comparison Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Performance Metrics Comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "metrics_df = results_comparison.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "metrics_df.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "plt.title('üöÄ Advanced Models - Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2-4. Confusion Matrices\n",
    "models_pred = [\n",
    "    ('XGBoost', y_pred_xgb),\n",
    "    ('Neural Network', y_pred_nn),\n",
    "    ('Hybrid Ensemble', y_pred_ensemble)\n",
    "]\n",
    "\n",
    "for i, (name, y_pred) in enumerate(models_pred):\n",
    "    plt.subplot(2, 3, i + 2)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    plt.title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "\n",
    "# 5. ROC Curves\n",
    "plt.subplot(2, 3, 5)\n",
    "models_roc = [\n",
    "    ('XGBoost', y_pred_proba_xgb, '#1e3a8a'),\n",
    "    ('Neural Network', y_pred_proba_nn, '#059669'),\n",
    "    ('Hybrid Ensemble', y_pred_proba_ensemble, '#d97706')\n",
    "]\n",
    "\n",
    "for name, y_proba, color in models_roc:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, color=color, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('üìà ROC Curves - Advanced Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# 6. Model Performance Radar Chart\n",
    "plt.subplot(2, 3, 6, projection='polar')\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "N = len(categories)\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for idx, row in results_comparison.iterrows():\n",
    "    values = row[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].tolist()\n",
    "    values += values[:1]\n",
    "    plt.plot(angles, values, 'o-', linewidth=2, label=row['Model'])\n",
    "    plt.fill(angles, values, alpha=0.15)\n",
    "\n",
    "plt.xticks(angles[:-1], categories)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('üéØ Performance Radar', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization dashboard created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ **Save Advanced Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ SAVING ADVANCED MODELS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create directory for advanced models\n",
    "advanced_models_dir = 'models/advanced_models'\n",
    "os.makedirs(advanced_models_dir, exist_ok=True)\n",
    "\n",
    "# Save XGBoost\n",
    "joblib.dump(best_xgb, f'{advanced_models_dir}/xgboost_model.pkl')\n",
    "print(\"‚úÖ Saved XGBoost model\")\n",
    "\n",
    "# Save Neural Network\n",
    "nn_model.save(f'{advanced_models_dir}/neural_network_model.h5')\n",
    "print(\"‚úÖ Saved Neural Network model\")\n",
    "\n",
    "# Save Hybrid Ensemble\n",
    "joblib.dump(ensemble_model, f'{advanced_models_dir}/hybrid_ensemble_model.pkl')\n",
    "print(\"‚úÖ Saved Hybrid Ensemble model\")\n",
    "\n",
    "# Save performance results\n",
    "results_comparison.to_csv(f'{advanced_models_dir}/advanced_results.csv', index=False)\n",
    "print(\"‚úÖ Saved performance results\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(f'{advanced_models_dir}/xgboost_feature_importance.csv', index=False)\n",
    "print(\"‚úÖ Saved feature importance\")\n",
    "\n",
    "print(f\"\\nüìÅ All models saved in '{advanced_models_dir}' directory\")\n",
    "print(\"\\n‚úÖ CardioFusion advanced models training completed successfully!\")\n",
    "print(\"üöÄ Ready for SHAP explainability and web application deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **Training Summary**\n",
    "\n",
    "### üéØ **Key Achievements**\n",
    "\n",
    "1. **XGBoost Model**\n",
    "   - Implemented with hyperparameter optimization\n",
    "   - Achieved superior performance through gradient boosting\n",
    "   - Feature importance analysis completed\n",
    "\n",
    "2. **Neural Network**\n",
    "   - Professional deep learning architecture\n",
    "   - Batch normalization and dropout for regularization\n",
    "   - Early stopping and learning rate scheduling\n",
    "\n",
    "3. **Hybrid Ensemble**\n",
    "   - Combines best of all models\n",
    "   - Weighted soft voting for optimal predictions\n",
    "   - Likely best overall performance\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "\n",
    "1. ‚úÖ Implement SHAP explainability\n",
    "2. ‚úÖ Build Streamlit web application\n",
    "3. ‚úÖ Create prediction widget for Jupyter\n",
    "4. ‚úÖ Deploy to production\n",
    "\n",
    "---\n",
    "\n",
    "*CardioFusion - Professional ML for Heart Disease Prediction* ü©∫"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
